{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, pickle, scipy.sparse, lightgbm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# DATA_DIR = \"/kaggle/input/open-problems-multimodal/\"\n",
    "DATA_DIR = \"/home/wuxinchao/data/project/kaggle_comp_scmo_data/data\"\n",
    "FP_CELL_METADATA = os.path.join(DATA_DIR,\"metadata.csv\")\n",
    "\n",
    "FP_CITE_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_cite_inputs.h5\")\n",
    "FP_CITE_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_cite_targets.h5\")\n",
    "FP_CITE_TEST_INPUTS = os.path.join(DATA_DIR,\"test_cite_inputs.h5\")\n",
    "\n",
    "FP_MULTIOME_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_multi_inputs.h5\")\n",
    "FP_MULTIOME_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_multi_targets.h5\")\n",
    "FP_MULTIOME_TEST_INPUTS = os.path.join(DATA_DIR,\"test_multi_inputs.h5\")\n",
    "\n",
    "FP_SUBMISSION = os.path.join(DATA_DIR,\"sample_submission.csv\")\n",
    "FP_EVALUATION_IDS = os.path.join(DATA_DIR,\"evaluation_ids.csv\")\n",
    "\n",
    "CROSS_VALIDATE = True\n",
    "SUBMIT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_score(y_true, y_pred):\n",
    "    \"\"\"Scores the predictions according to the competition rules. \n",
    "    \n",
    "    It is assumed that the predictions are not constant.\n",
    "    \n",
    "    Returns the average of each sample's Pearson correlation coefficient\"\"\"\n",
    "    if type(y_true) == pd.DataFrame: y_true = y_true.values\n",
    "    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119651, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(FP_CELL_METADATA, index_col='cell_id')\n",
    "metadata_df = metadata_df[metadata_df.technology==\"citeseq\"]\n",
    "metadata_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wuxinchao/data/project/kaggle_comp_scmo_data/data/train_cite_inputs.h5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP_CITE_TRAIN_INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X shape: (70988, 20856) 5.515 GByte\n",
      "Original Xt shape: (48663, 20856) 3.781 GByte\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_hdf(FP_CITE_TRAIN_INPUTS)\n",
    "X_test = pd.read_hdf(FP_CITE_TEST_INPUTS)\n",
    "constant_cols = list(X.columns[(X == 0).all(axis=0).values]) + list(X_test.columns[(X_test == 0).all(axis=0).values])\n",
    "X = X.drop(constant_cols, axis=1)\n",
    "cell_idx = X.index\n",
    "meta = metadata_df.reindex(cell_idx)\n",
    "\n",
    "important_cols = []\n",
    "Y = pd.read_hdf(FP_CITE_TRAIN_TARGETS)\n",
    "for y_col in Y.columns:\n",
    "    important_cols += [x_col for x_col in X.columns if y_col in x_col]\n",
    "X0 = X[important_cols].values\n",
    "print(f\"Original X shape: {str(X.shape):14} {X.size*4/1024/1024/1024:2.3f} GByte\")\n",
    "gc.collect()\n",
    "X = scipy.sparse.csr_matrix(X.values)\n",
    "gc.collect()\n",
    "\n",
    "# Read test and convert to sparse matrix\n",
    "Xt = pd.read_hdf(FP_CITE_TEST_INPUTS).drop(columns=constant_cols)\n",
    "cell_index_test = Xt.index\n",
    "meta_test = metadata_df.reindex(cell_index_test)\n",
    "X0t = Xt[important_cols].values\n",
    "print(f\"Original Xt shape: {str(Xt.shape):14} {Xt.size*4/1024/1024/1024:2.3f} GByte\")\n",
    "gc.collect()\n",
    "Xt = scipy.sparse.csr_matrix(Xt.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of both before SVD: (119651, 20856)\n",
      "Shape of both after SVD:  (119651, 512)\n",
      "Reduced X shape:  (70988, 656)   0.173 GByte\n",
      "Reduced Xt shape: (48663, 656)   0.119 GByte\n"
     ]
    }
   ],
   "source": [
    "# Apply the singular value decomposition\n",
    "both = scipy.sparse.vstack([X, Xt])\n",
    "assert both.shape[0] == 119651\n",
    "print(f\"Shape of both before SVD: {both.shape}\")\n",
    "svd = TruncatedSVD(n_components=512, random_state=1) # 512\n",
    "both = svd.fit_transform(both)\n",
    "print(f\"Shape of both after SVD:  {both.shape}\")\n",
    "\n",
    "# Hstack the svd output with the important features\n",
    "X = both[:70988]\n",
    "Xt = both[70988:]\n",
    "del both\n",
    "X = np.hstack([X, X0])\n",
    "Xt = np.hstack([Xt, X0t])\n",
    "print(f\"Reduced X shape:  {str(X.shape):14} {X.size*4/1024/1024/1024:2.3f} GByte\")\n",
    "print(f\"Reduced Xt shape: {str(Xt.shape):14} {Xt.size*4/1024/1024/1024:2.3f} GByte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y shape: (70988, 140)   0.037 GByte\n"
     ]
    }
   ],
   "source": [
    "Y = pd.read_hdf(FP_CITE_TRAIN_TARGETS)\n",
    "y_columns = list(Y.columns)\n",
    "Y = Y.values\n",
    "\n",
    "print(f\"Y shape: {str(Y.shape):14} {Y.size*4/1024/1024/1024:2.3f} GByte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_params = {\n",
    "     'learning_rate': 0.1, \n",
    "     'max_depth': 10, \n",
    "     'num_leaves': 200,\n",
    "     'min_child_samples': 250,\n",
    "     'colsample_bytree': 0.8, \n",
    "     'subsample': 0.6, \n",
    "     \"seed\": 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CROSS_VALIDATE:\n",
    "    y_cols = Y.shape[1] # set this to a small number for a quick test\n",
    "    n_estimators = 300\n",
    "\n",
    "    kf = GroupKFold(n_splits=3)\n",
    "    score_list = []\n",
    "    for fold, (idx_tr, idx_va) in enumerate(kf.split(X, groups=meta.donor)):\n",
    "        model = None\n",
    "        gc.collect()\n",
    "        X_tr = X[idx_tr]\n",
    "        y_tr = Y[:,:y_cols][idx_tr]\n",
    "        X_va = X[idx_va]\n",
    "        y_va = Y[:,:y_cols][idx_va]\n",
    "\n",
    "        models, va_preds = [], []\n",
    "        for i in range(y_cols):\n",
    "            #print(f\"Training column {i:3} for validation\")\n",
    "            model = lightgbm.LGBMRegressor(n_estimators=n_estimators, **lightgbm_params)\n",
    "            # models.append(model) # not needed\n",
    "            model.fit(X_tr, y_tr[:,i].copy())\n",
    "            va_preds.append(model.predict(X_va))\n",
    "        y_va_pred = np.column_stack(va_preds) # concatenate the 140 predictions\n",
    "        del va_preds\n",
    "\n",
    "        del X_tr, y_tr, X_va\n",
    "        gc.collect()\n",
    "\n",
    "        # We validate the model (mse and correlation over all 140 columns)\n",
    "        mse = mean_squared_error(y_va, y_va_pred)\n",
    "        corrscore = correlation_score(y_va, y_va_pred)\n",
    "        \n",
    "        del y_va\n",
    "\n",
    "        print(f\"Fold {fold} {X.shape[1]:4}: mse = {mse:.5f}, corr =  {corrscore:.5f}\")\n",
    "        score_list.append((mse, corrscore))\n",
    "        break # We only need the first fold\n",
    "\n",
    "    if len(score_list) > 1:\n",
    "        # Show overall score\n",
    "        result_df = pd.DataFrame(score_list, columns=['mse', 'corrscore'])\n",
    "        print(f\"Average LGBM mse = {result_df.mse.mean():.5f}; corr = {result_df.corrscore.mean():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_preds = []\n",
    "n_estimators = 300\n",
    "y_cols = Y.shape[1]\n",
    "for i in range(y_cols):\n",
    "    print(f\"Training column {i:3} for test\")\n",
    "    model = lightgbm.LGBMRegressor(n_estimators=n_estimators, **lightgbm_params)\n",
    "    model.fit(X, Y[:,i].copy())\n",
    "    te_preds.append(model.predict(Xt))\n",
    "y_te_pred = np.column_stack(te_preds)\n",
    "del te_preds\n",
    "\n",
    "print(f\"Test_pred shape: {str(y_te_pred.shape):14}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9a3d897ef0b1e7415fe4468808571913e41281b79a56511723d411ccb064e7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
